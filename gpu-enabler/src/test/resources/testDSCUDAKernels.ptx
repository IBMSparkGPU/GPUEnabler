//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21313570
// Cuda compilation tools, release 8.0, V8.0.53
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_35
.address_size 64

	// .globl	identity
.extern .func __assertfail
(
	.param .b64 __assertfail_param_0,
	.param .b64 __assertfail_param_1,
	.param .b32 __assertfail_param_2,
	.param .b64 __assertfail_param_3,
	.param .b64 __assertfail_param_4
)
;
.global .align 1 .b8 __T20[44] = {118, 111, 105, 100, 32, 115, 117, 109, 40, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 41, 0};
.global .align 1 .b8 __T21[65] = {118, 111, 105, 100, 32, 105, 110, 116, 65, 114, 114, 97, 121, 83, 117, 109, 40, 105, 110, 116, 32, 42, 44, 32, 99, 111, 110, 115, 116, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 41, 0};
.global .align 1 .b8 __T22[75] = {118, 111, 105, 100, 32, 68, 97, 116, 97, 80, 111, 105, 110, 116, 82, 101, 100, 117, 99, 101, 40, 105, 110, 116, 32, 42, 44, 32, 99, 111, 110, 115, 116, 32, 100, 111, 117, 98, 108, 101, 32, 42, 44, 32, 100, 111, 117, 98, 108, 101, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 41, 0};
.global .align 1 .b8 $str[31] = {106, 117, 109, 112, 32, 61, 61, 32, 98, 108, 111, 99, 107, 68, 105, 109, 46, 120, 32, 42, 32, 103, 114, 105, 100, 68, 105, 109, 46, 120, 0};
.global .align 1 .b8 $str1[21] = {116, 101, 115, 116, 68, 83, 67, 85, 68, 65, 75, 101, 114, 110, 101, 108, 115, 46, 99, 117, 0};

.visible .entry identity(
	.param .u32 identity_param_0,
	.param .u64 identity_param_1,
	.param .u64 identity_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [identity_param_1];
	ld.param.u64 	%rd3, [identity_param_2];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd4, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd5, %r3, %r2;
	add.s64 	%rd1, %rd5, %rd4;
	ld.param.s32 	%rd6, [identity_param_0];
	setp.ge.s64	%p1, %rd1, %rd6;
	@%p1 bra 	BB0_2;

	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u64 	%rd10, [%rd9];
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd12, %rd11, %rd8;
	st.global.u64 	[%rd12], %rd10;

BB0_2:
	ret;
}

	// .globl	intArrayIdentity
.visible .entry intArrayIdentity(
	.param .u32 intArrayIdentity_param_0,
	.param .u64 intArrayIdentity_param_1,
	.param .u64 intArrayIdentity_param_2,
	.param .u32 intArrayIdentity_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<19>;


	ld.param.u32 	%r5, [intArrayIdentity_param_0];
	ld.param.u64 	%rd10, [intArrayIdentity_param_1];
	ld.param.u64 	%rd11, [intArrayIdentity_param_2];
	ld.param.u32 	%r4, [intArrayIdentity_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r6, %r1, %r2, %r3;
	setp.ge.s32	%p1, %r6, %r5;
	@%p1 bra 	BB1_4;

	setp.lt.s32	%p2, %r4, 1;
	@%p2 bra 	BB1_4;

	cvta.to.global.u64 	%rd13, %rd11;
	cvta.to.global.u64 	%rd14, %rd10;
	cvt.s64.s32	%rd1, %r4;
	mul.lo.s32 	%r8, %r4, %r6;
	mul.wide.s32 	%rd15, %r8, 4;
	add.s64 	%rd17, %rd13, %rd15;
	add.s64 	%rd16, %rd14, %rd15;
	mov.u64 	%rd18, 0;

BB1_3:
	ld.global.u32 	%r9, [%rd16];
	st.global.u32 	[%rd17], %r9;
	add.s64 	%rd17, %rd17, 4;
	add.s64 	%rd16, %rd16, 4;
	add.s64 	%rd18, %rd18, 1;
	setp.lt.s64	%p3, %rd18, %rd1;
	@%p3 bra 	BB1_3;

BB1_4:
	ret;
}

	// .globl	IntDataPointIdentity
.visible .entry IntDataPointIdentity(
	.param .u32 IntDataPointIdentity_param_0,
	.param .u64 IntDataPointIdentity_param_1,
	.param .u64 IntDataPointIdentity_param_2,
	.param .u64 IntDataPointIdentity_param_3,
	.param .u64 IntDataPointIdentity_param_4,
	.param .u32 IntDataPointIdentity_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd11, [IntDataPointIdentity_param_1];
	ld.param.u64 	%rd12, [IntDataPointIdentity_param_2];
	ld.param.u64 	%rd13, [IntDataPointIdentity_param_3];
	ld.param.u64 	%rd14, [IntDataPointIdentity_param_4];
	ld.param.u32 	%r4, [IntDataPointIdentity_param_5];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd15, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd16, %r3, %r2;
	add.s64 	%rd1, %rd16, %rd15;
	ld.param.s32 	%rd17, [IntDataPointIdentity_param_0];
	setp.ge.s64	%p1, %rd1, %rd17;
	@%p1 bra 	BB2_5;

	setp.lt.s32	%p2, %r4, 1;
	@%p2 bra 	BB2_4;

	cvta.to.global.u64 	%rd19, %rd13;
	cvta.to.global.u64 	%rd20, %rd11;
	cvt.s64.s32	%rd2, %r4;
	mul.lo.s64 	%rd24, %rd1, %rd2;
	shl.b64 	%rd25, %rd24, 2;
	add.s64 	%rd32, %rd19, %rd25;
	add.s64 	%rd31, %rd20, %rd25;
	mov.u64 	%rd33, 0;

BB2_3:
	ld.global.u32 	%r5, [%rd31];
	st.global.u32 	[%rd32], %r5;
	add.s64 	%rd32, %rd32, 4;
	add.s64 	%rd31, %rd31, 4;
	add.s64 	%rd33, %rd33, 1;
	setp.lt.s64	%p3, %rd33, %rd2;
	@%p3 bra 	BB2_3;

BB2_4:
	cvta.to.global.u64 	%rd26, %rd14;
	cvta.to.global.u64 	%rd27, %rd12;
	shl.b64 	%rd28, %rd1, 2;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.u32 	%r6, [%rd29];
	add.s64 	%rd30, %rd26, %rd28;
	st.global.u32 	[%rd30], %r6;

BB2_5:
	ret;
}

	// .globl	intArrayAdd
.visible .entry intArrayAdd(
	.param .u64 intArrayAdd_param_0,
	.param .u64 intArrayAdd_param_1,
	.param .u64 intArrayAdd_param_2,
	.param .u64 intArrayAdd_param_3,
	.param .u64 intArrayAdd_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd16, [intArrayAdd_param_0];
	ld.param.u64 	%rd13, [intArrayAdd_param_1];
	ld.param.u64 	%rd14, [intArrayAdd_param_2];
	ld.param.u64 	%rd15, [intArrayAdd_param_3];
	ld.param.u64 	%rd17, [intArrayAdd_param_4];
	cvta.to.global.u64 	%rd1, %rd17;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd18, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd19, %r3, %r2;
	add.s64 	%rd20, %rd19, %rd18;
	cvta.to.global.u64 	%rd21, %rd16;
	ld.global.s32 	%rd22, [%rd21];
	setp.ge.s64	%p1, %rd20, %rd22;
	@%p1 bra 	BB3_4;

	ld.global.u32 	%r4, [%rd1];
	setp.lt.s32	%p2, %r4, 1;
	@%p2 bra 	BB3_4;

	cvta.to.global.u64 	%rd24, %rd14;
	cvta.to.global.u64 	%rd35, %rd15;
	cvta.to.global.u64 	%rd25, %rd13;
	cvt.s64.s32	%rd26, %r4;
	mul.lo.s64 	%rd30, %rd20, %rd26;
	shl.b64 	%rd31, %rd30, 2;
	add.s64 	%rd34, %rd24, %rd31;
	add.s64 	%rd33, %rd25, %rd31;
	mov.u64 	%rd36, 0;

BB3_3:
	ld.global.u32 	%r5, [%rd35];
	ld.global.u32 	%r6, [%rd33];
	add.s32 	%r7, %r5, %r6;
	st.global.u32 	[%rd34], %r7;
	ld.global.s32 	%rd32, [%rd1];
	add.s64 	%rd35, %rd35, 4;
	add.s64 	%rd34, %rd34, 4;
	add.s64 	%rd33, %rd33, 4;
	add.s64 	%rd36, %rd36, 1;
	setp.lt.s64	%p3, %rd36, %rd32;
	@%p3 bra 	BB3_3;

BB3_4:
	ret;
}

	// .globl	vectorLength
.visible .entry vectorLength(
	.param .u64 vectorLength_param_0,
	.param .u64 vectorLength_param_1,
	.param .u64 vectorLength_param_2,
	.param .u64 vectorLength_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd5, [vectorLength_param_0];
	ld.param.u64 	%rd2, [vectorLength_param_1];
	ld.param.u64 	%rd3, [vectorLength_param_2];
	ld.param.u64 	%rd4, [vectorLength_param_3];
	cvta.to.global.u64 	%rd6, %rd5;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd7, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd8, %r3, %r2;
	add.s64 	%rd1, %rd8, %rd7;
	ld.global.s32 	%rd9, [%rd6];
	setp.ge.s64	%p1, %rd1, %rd9;
	@%p1 bra 	BB4_2;

	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 3;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.f64 	%fd1, [%rd12];
	cvta.to.global.u64 	%rd13, %rd3;
	add.s64 	%rd14, %rd13, %rd11;
	ld.global.f64 	%fd2, [%rd14];
	mul.f64 	%fd3, %fd2, %fd2;
	fma.rn.f64 	%fd4, %fd1, %fd1, %fd3;
	sqrt.rn.f64 	%fd5, %fd4;
	cvta.to.global.u64 	%rd15, %rd4;
	add.s64 	%rd16, %rd15, %rd11;
	st.global.f64 	[%rd16], %fd5;

BB4_2:
	ret;
}

	// .globl	plusMinus
.visible .entry plusMinus(
	.param .u64 plusMinus_param_0,
	.param .u64 plusMinus_param_1,
	.param .u64 plusMinus_param_2,
	.param .u64 plusMinus_param_3,
	.param .u64 plusMinus_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<4>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd6, [plusMinus_param_0];
	ld.param.u64 	%rd2, [plusMinus_param_1];
	ld.param.u64 	%rd3, [plusMinus_param_2];
	ld.param.u64 	%rd4, [plusMinus_param_3];
	ld.param.u64 	%rd5, [plusMinus_param_4];
	cvta.to.global.u64 	%rd7, %rd6;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd8, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd9, %r3, %r2;
	add.s64 	%rd1, %rd9, %rd8;
	ld.global.s32 	%rd10, [%rd7];
	setp.ge.s64	%p1, %rd1, %rd10;
	@%p1 bra 	BB5_2;

	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 3;
	add.s64 	%rd13, %rd11, %rd12;
	cvta.to.global.u64 	%rd14, %rd3;
	shl.b64 	%rd15, %rd1, 2;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.f32 	%f1, [%rd16];
	cvt.f64.f32	%fd1, %f1;
	ld.global.f64 	%fd2, [%rd13];
	sub.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd17, %rd4;
	add.s64 	%rd18, %rd17, %rd12;
	st.global.f64 	[%rd18], %fd3;
	ld.global.f32 	%f2, [%rd16];
	cvt.f64.f32	%fd4, %f2;
	ld.global.f64 	%fd5, [%rd13];
	add.f64 	%fd6, %fd5, %fd4;
	cvt.rn.f32.f64	%f3, %fd6;
	cvta.to.global.u64 	%rd19, %rd5;
	add.s64 	%rd20, %rd19, %rd15;
	st.global.f32 	[%rd20], %f3;

BB5_2:
	ret;
}

	// .globl	applyLinearFunction
.visible .entry applyLinearFunction(
	.param .u64 applyLinearFunction_param_0,
	.param .u64 applyLinearFunction_param_1,
	.param .u64 applyLinearFunction_param_2,
	.param .u64 applyLinearFunction_param_3,
	.param .u64 applyLinearFunction_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd6, [applyLinearFunction_param_0];
	ld.param.u64 	%rd2, [applyLinearFunction_param_1];
	ld.param.u64 	%rd3, [applyLinearFunction_param_2];
	ld.param.u64 	%rd4, [applyLinearFunction_param_3];
	ld.param.u64 	%rd5, [applyLinearFunction_param_4];
	cvta.to.global.u64 	%rd7, %rd6;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd8, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd9, %r3, %r2;
	add.s64 	%rd1, %rd9, %rd8;
	ld.global.s32 	%rd10, [%rd7];
	setp.ge.s64	%p1, %rd1, %rd10;
	@%p1 bra 	BB6_2;

	cvta.to.global.u64 	%rd11, %rd4;
	ld.global.u16 	%r4, [%rd11];
	cvta.to.global.u64 	%rd12, %rd5;
	ld.global.s16 	%r5, [%rd12];
	cvta.to.global.u64 	%rd13, %rd2;
	shl.b64 	%rd14, %rd1, 1;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.s16 	%r6, [%rd15];
	mad.lo.s32 	%r7, %r6, %r5, %r4;
	cvta.to.global.u64 	%rd16, %rd3;
	add.s64 	%rd17, %rd16, %rd14;
	st.global.u16 	[%rd17], %r7;

BB6_2:
	ret;
}

	// .globl	blockXOR
.visible .entry blockXOR(
	.param .u64 blockXOR_param_0,
	.param .u64 blockXOR_param_1,
	.param .u64 blockXOR_param_2,
	.param .u64 blockXOR_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd5, [blockXOR_param_0];
	ld.param.u64 	%rd2, [blockXOR_param_1];
	ld.param.u64 	%rd3, [blockXOR_param_2];
	ld.param.u64 	%rd4, [blockXOR_param_3];
	cvta.to.global.u64 	%rd6, %rd5;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd7, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd8, %r3, %r2;
	add.s64 	%rd1, %rd8, %rd7;
	shl.b64 	%rd9, %rd1, 3;
	ld.global.s32 	%rd10, [%rd6];
	setp.ge.s64	%p1, %rd9, %rd10;
	@%p1 bra 	BB7_2;

	cvta.to.global.u64 	%rd11, %rd4;
	cvta.to.global.u64 	%rd12, %rd2;
	add.s64 	%rd14, %rd12, %rd9;
	ld.global.u64 	%rd15, [%rd11];
	ld.global.u64 	%rd16, [%rd14];
	xor.b64  	%rd17, %rd15, %rd16;
	cvta.to.global.u64 	%rd18, %rd3;
	add.s64 	%rd19, %rd18, %rd9;
	st.global.u64 	[%rd19], %rd17;

BB7_2:
	ret;
}

	// .globl	multiplyBy2
.visible .entry multiplyBy2(
	.param .u64 multiplyBy2_param_0,
	.param .u64 multiplyBy2_param_1,
	.param .u64 multiplyBy2_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [multiplyBy2_param_0];
	ld.param.u64 	%rd1, [multiplyBy2_param_1];
	ld.param.u64 	%rd2, [multiplyBy2_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r1, %r2, %r3, %r4;
	ld.global.u32 	%r5, [%rd4];
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB8_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r6, [%rd7];
	shl.b32 	%r7, %r6, 1;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd6;
	st.global.u32 	[%rd9], %r7;

BB8_2:
	ret;
}

	// .globl	multiplyBy2_self
.visible .entry multiplyBy2_self(
	.param .u64 multiplyBy2_self_param_0,
	.param .u64 multiplyBy2_self_param_1,
	.param .u64 multiplyBy2_self_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [multiplyBy2_self_param_0];
	ld.param.u64 	%rd1, [multiplyBy2_self_param_1];
	ld.param.u64 	%rd2, [multiplyBy2_self_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r1, %r2, %r3, %r4;
	ld.global.u32 	%r5, [%rd4];
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB9_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r6, [%rd7];
	shl.b32 	%r7, %r6, 1;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd6;
	st.global.u32 	[%rd9], %r7;
	st.global.u32 	[%rd7], %r7;

BB9_2:
	ret;
}

	// .globl	sum
.visible .entry sum(
	.param .u64 sum_param_0,
	.param .u64 sum_param_1,
	.param .u64 sum_param_2,
	.param .u64 sum_param_3,
	.param .u64 sum_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd16, [sum_param_0];
	ld.param.u64 	%rd17, [sum_param_1];
	ld.param.u64 	%rd15, [sum_param_2];
	ld.param.u64 	%rd18, [sum_param_3];
	cvta.to.global.u64 	%rd39, %rd17;
	cvta.to.global.u64 	%rd2, %rd16;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd19, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd20, %r3, %r2;
	add.s64 	%rd3, %rd20, %rd19;
	cvta.to.global.u64 	%rd21, %rd18;
	ld.global.u32 	%r13, [%rd21];
	setp.eq.s32	%p1, %r13, 0;
	@%p1 bra 	BB10_5;

	setp.ne.s64	%p2, %rd3, 0;
	@%p2 bra 	BB10_12;

	ld.global.s32 	%rd23, [%rd2];
	setp.lt.s64	%p3, %rd23, 16384;
	selp.b64	%rd4, %rd23, 16384, %p3;
	mov.u32 	%r24, 0;
	mov.u32 	%r25, %r24;
	mov.u64 	%rd40, 0;
	setp.lt.s64	%p4, %rd4, 1;
	@%p4 bra 	BB10_4;

BB10_3:
	ld.global.u32 	%r16, [%rd39];
	add.s32 	%r25, %r16, %r25;
	add.s64 	%rd39, %rd39, 4;
	add.s64 	%rd40, %rd40, 1;
	setp.lt.s64	%p5, %rd40, %rd4;
	mov.u32 	%r24, %r25;
	@%p5 bra 	BB10_3;

BB10_4:
	cvta.to.global.u64 	%rd24, %rd15;
	st.global.u32 	[%rd24], %r24;
	bra.uni 	BB10_12;

BB10_5:
	ld.global.u32 	%r26, [%rd2];
	cvt.s64.s32	%rd25, %r26;
	setp.ge.s64	%p6, %rd3, %rd25;
	@%p6 bra 	BB10_12;

	mov.u32 	%r17, %nctaid.x;
	mul.lo.s32 	%r18, %r17, %r3;
	setp.eq.s32	%p7, %r18, 16384;
	@%p7 bra 	BB10_8;

	mov.u64 	%rd26, $str;
	cvta.global.u64 	%rd27, %rd26;
	mov.u64 	%rd28, $str1;
	cvta.global.u64 	%rd29, %rd28;
	mov.u64 	%rd30, __T20;
	cvta.global.u64 	%rd31, %rd30;
	mov.u32 	%r19, 137;
	mov.u64 	%rd32, 1;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd27;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd29;
	.param .b32 param2;
	st.param.b32	[param2+0], %r19;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd31;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd32;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 0
	ld.global.u32 	%r26, [%rd2];

BB10_8:
	cvt.s64.s32	%rd9, %r26;
	mov.u32 	%r27, 0;
	setp.ge.s64	%p8, %rd3, %rd9;
	@%p8 bra 	BB10_11;

	add.s64 	%rd35, %rd20, %rd19;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd41, %rd39, %rd36;
	mov.u32 	%r27, 0;
	mov.u64 	%rd42, %rd3;

BB10_10:
	mov.u64 	%rd12, %rd42;
	ld.global.u32 	%r22, [%rd41];
	add.s32 	%r27, %r22, %r27;
	add.s64 	%rd41, %rd41, 65536;
	add.s64 	%rd14, %rd12, 16384;
	setp.lt.s64	%p9, %rd14, %rd9;
	mov.u64 	%rd42, %rd14;
	@%p9 bra 	BB10_10;

BB10_11:
	shl.b64 	%rd37, %rd3, 2;
	add.s64 	%rd38, %rd39, %rd37;
	st.global.u32 	[%rd38], %r27;

BB10_12:
	ret;
}

	// .globl	intArraySum
.visible .entry intArraySum(
	.param .u64 intArraySum_param_0,
	.param .u64 intArraySum_param_1,
	.param .u64 intArraySum_param_2,
	.param .u64 intArraySum_param_3,
	.param .u64 intArraySum_param_4,
	.param .u64 intArraySum_param_5
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<90>;


	ld.param.u64 	%rd35, [intArraySum_param_0];
	ld.param.u64 	%rd36, [intArraySum_param_1];
	ld.param.u64 	%rd37, [intArraySum_param_2];
	ld.param.u64 	%rd38, [intArraySum_param_3];
	ld.param.u64 	%rd39, [intArraySum_param_4];
	cvta.to.global.u64 	%rd1, %rd37;
	cvta.to.global.u64 	%rd2, %rd36;
	cvta.to.global.u64 	%rd3, %rd38;
	cvta.to.global.u64 	%rd4, %rd35;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd40, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd41, %r3, %r2;
	add.s64 	%rd5, %rd41, %rd40;
	cvta.to.global.u64 	%rd42, %rd39;
	ld.global.u32 	%r20, [%rd42];
	setp.eq.s32	%p1, %r20, 0;
	@%p1 bra 	BB11_10;

	setp.ne.s64	%p2, %rd5, 0;
	@%p2 bra 	BB11_20;

	ld.global.s32 	%rd43, [%rd4];
	setp.lt.s64	%p3, %rd43, 16384;
	selp.b64	%rd6, %rd43, 16384, %p3;
	setp.lt.s64	%p4, %rd6, 1;
	@%p4 bra 	BB11_20;

	ld.global.u32 	%r36, [%rd3];
	mov.u64 	%rd44, 0;
	mov.u64 	%rd79, %rd44;

BB11_4:
	mov.u32 	%r32, %r36;
	mov.u32 	%r5, %r32;
	setp.gt.s32	%p5, %r5, 0;
	setp.eq.s64	%p6, %rd79, 0;
	and.pred  	%p7, %p6, %p5;
	mov.u64 	%rd78, %rd44;
	mov.u32 	%r34, %r5;
	mov.u64 	%rd83, %rd1;
	@!%p7 bra 	BB11_6;
	bra.uni 	BB11_5;

BB11_5:
	mov.u64 	%rd8, %rd83;
	mov.u64 	%rd9, %rd78;
	mov.u32 	%r21, 0;
	st.global.u32 	[%rd8], %r21;
	ld.global.u32 	%r6, [%rd3];
	cvt.s64.s32	%rd46, %r6;
	add.s64 	%rd10, %rd8, 4;
	add.s64 	%rd11, %rd9, 1;
	setp.lt.s64	%p8, %rd11, %rd46;
	mov.u64 	%rd78, %rd11;
	mov.u32 	%r34, %r6;
	mov.u64 	%rd83, %rd10;
	@%p8 bra 	BB11_5;

BB11_6:
	mov.u32 	%r31, %r34;
	mov.u32 	%r35, %r31;
	setp.lt.s32	%p9, %r35, 1;
	@%p9 bra 	BB11_9;

	cvt.s64.s32	%rd48, %r5;
	mul.lo.s64 	%rd49, %rd79, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd80, %rd2, %rd50;
	mov.u64 	%rd84, 0;
	mov.u64 	%rd82, %rd1;

BB11_8:
	ld.global.u32 	%r22, [%rd82];
	ld.global.u32 	%r23, [%rd80];
	add.s32 	%r24, %r22, %r23;
	st.global.u32 	[%rd82], %r24;
	ld.global.u32 	%r35, [%rd3];
	cvt.s64.s32	%rd51, %r35;
	add.s64 	%rd82, %rd82, 4;
	add.s64 	%rd80, %rd80, 4;
	add.s64 	%rd84, %rd84, 1;
	setp.lt.s64	%p10, %rd84, %rd51;
	@%p10 bra 	BB11_8;

BB11_9:
	mov.u32 	%r36, %r35;
	add.s64 	%rd79, %rd79, 1;
	setp.lt.s64	%p11, %rd79, %rd6;
	@%p11 bra 	BB11_4;
	bra.uni 	BB11_20;

BB11_10:
	ld.global.u32 	%r37, [%rd4];
	cvt.s64.s32	%rd52, %r37;
	setp.ge.s64	%p12, %rd5, %rd52;
	@%p12 bra 	BB11_20;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r26, %r25, %r3;
	setp.eq.s32	%p13, %r26, 16384;
	@%p13 bra 	BB11_13;

	mov.u64 	%rd53, $str;
	cvta.global.u64 	%rd54, %rd53;
	mov.u64 	%rd55, $str1;
	cvta.global.u64 	%rd56, %rd55;
	mov.u64 	%rd57, __T21;
	cvta.global.u64 	%rd58, %rd57;
	mov.u32 	%r27, 161;
	mov.u64 	%rd59, 1;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd56;
	.param .b32 param2;
	st.param.b32	[param2+0], %r27;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd58;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd59;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 1
	ld.global.u32 	%r37, [%rd4];

BB11_13:
	ld.global.u32 	%r38, [%rd3];
	cvt.s64.s32	%rd60, %r37;
	add.s64 	%rd86, %rd5, 16384;
	setp.ge.s64	%p14, %rd86, %rd60;
	@%p14 bra 	BB11_20;

	cvt.s64.s32	%rd62, %r38;
	add.s64 	%rd22, %rd5, 16384;
	mul.lo.s64 	%rd65, %rd5, %rd62;
	shl.b64 	%rd66, %rd65, 2;
	add.s64 	%rd23, %rd2, %rd66;
	mov.u64 	%rd85, 0;

BB11_15:
	setp.lt.s32	%p15, %r38, 1;
	@%p15 bra 	BB11_19;

	shl.b64 	%rd68, %rd85, 14;
	add.s64 	%rd69, %rd22, %rd68;
	shl.b64 	%rd70, %rd69, 2;
	add.s64 	%rd71, %rd2, %rd70;
	cvt.s64.s32	%rd72, %r38;
	mul.lo.s64 	%rd73, %rd5, %rd72;
	shl.b64 	%rd74, %rd73, 2;
	add.s64 	%rd88, %rd71, %rd74;
	mov.u64 	%rd89, 0;
	mov.u64 	%rd87, %rd23;

BB11_17:
	mov.u64 	%rd27, %rd87;
	ld.global.u32 	%r28, [%rd27];
	ld.global.u32 	%r29, [%rd88];
	add.s32 	%r30, %r28, %r29;
	st.global.u32 	[%rd27], %r30;
	ld.global.u32 	%r38, [%rd3];
	cvt.s64.s32	%rd75, %r38;
	add.s64 	%rd88, %rd88, 4;
	add.s64 	%rd31, %rd27, 4;
	add.s64 	%rd89, %rd89, 1;
	setp.lt.s64	%p16, %rd89, %rd75;
	mov.u64 	%rd87, %rd31;
	@%p16 bra 	BB11_17;

	ld.global.u32 	%r37, [%rd4];

BB11_19:
	cvt.s64.s32	%rd76, %r37;
	add.s64 	%rd86, %rd86, 16384;
	setp.lt.s64	%p17, %rd86, %rd76;
	add.s64 	%rd85, %rd85, 1;
	@%p17 bra 	BB11_15;

BB11_20:
	ret;
}

	// .globl	DataPointMap
.visible .entry DataPointMap(
	.param .u64 DataPointMap_param_0,
	.param .u64 DataPointMap_param_1,
	.param .u64 DataPointMap_param_2,
	.param .u64 DataPointMap_param_3,
	.param .u64 DataPointMap_param_4,
	.param .u64 DataPointMap_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd16, [DataPointMap_param_0];
	ld.param.u64 	%rd13, [DataPointMap_param_1];
	ld.param.u64 	%rd14, [DataPointMap_param_3];
	ld.param.u64 	%rd15, [DataPointMap_param_4];
	ld.param.u64 	%rd17, [DataPointMap_param_5];
	cvta.to.global.u64 	%rd1, %rd17;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd18, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd19, %r3, %r2;
	add.s64 	%rd20, %rd19, %rd18;
	cvta.to.global.u64 	%rd21, %rd16;
	ld.global.s32 	%rd22, [%rd21];
	setp.ge.s64	%p1, %rd20, %rd22;
	@%p1 bra 	BB12_4;

	ld.global.u32 	%r4, [%rd1];
	setp.lt.s32	%p2, %r4, 1;
	@%p2 bra 	BB12_4;

	cvta.to.global.u64 	%rd24, %rd14;
	cvta.to.global.u64 	%rd35, %rd15;
	cvta.to.global.u64 	%rd25, %rd13;
	cvt.s64.s32	%rd26, %r4;
	mul.lo.s64 	%rd30, %rd20, %rd26;
	shl.b64 	%rd31, %rd30, 3;
	add.s64 	%rd34, %rd24, %rd31;
	add.s64 	%rd33, %rd25, %rd31;
	mov.u64 	%rd36, 0;

BB12_3:
	ld.global.f64 	%fd1, [%rd35];
	ld.global.f64 	%fd2, [%rd33];
	add.f64 	%fd3, %fd2, %fd1;
	st.global.f64 	[%rd34], %fd3;
	ld.global.s32 	%rd32, [%rd1];
	add.s64 	%rd35, %rd35, 8;
	add.s64 	%rd34, %rd34, 8;
	add.s64 	%rd33, %rd33, 8;
	add.s64 	%rd36, %rd36, 1;
	setp.lt.s64	%p3, %rd36, %rd32;
	@%p3 bra 	BB12_3;

BB12_4:
	ret;
}

	// .globl	DataPointReduce
.visible .entry DataPointReduce(
	.param .u64 DataPointReduce_param_0,
	.param .u64 DataPointReduce_param_1,
	.param .u64 DataPointReduce_param_2,
	.param .u64 DataPointReduce_param_3,
	.param .u64 DataPointReduce_param_4,
	.param .u64 DataPointReduce_param_5
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<32>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<91>;


	ld.param.u64 	%rd35, [DataPointReduce_param_0];
	ld.param.u64 	%rd36, [DataPointReduce_param_1];
	ld.param.u64 	%rd37, [DataPointReduce_param_2];
	ld.param.u64 	%rd38, [DataPointReduce_param_3];
	ld.param.u64 	%rd39, [DataPointReduce_param_4];
	cvta.to.global.u64 	%rd1, %rd37;
	cvta.to.global.u64 	%rd2, %rd36;
	cvta.to.global.u64 	%rd3, %rd38;
	cvta.to.global.u64 	%rd4, %rd35;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd40, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd41, %r3, %r2;
	add.s64 	%rd5, %rd41, %rd40;
	cvta.to.global.u64 	%rd42, %rd39;
	ld.global.u32 	%r20, [%rd42];
	setp.eq.s32	%p1, %r20, 0;
	@%p1 bra 	BB13_10;

	setp.ne.s64	%p2, %rd5, 0;
	@%p2 bra 	BB13_20;

	ld.global.s32 	%rd43, [%rd4];
	setp.lt.s64	%p3, %rd43, 16384;
	selp.b64	%rd6, %rd43, 16384, %p3;
	setp.lt.s64	%p4, %rd6, 1;
	@%p4 bra 	BB13_20;

	ld.global.u32 	%r29, [%rd3];
	mov.u64 	%rd44, 0;
	mov.u64 	%rd80, %rd44;

BB13_4:
	mov.u32 	%r25, %r29;
	mov.u32 	%r5, %r25;
	setp.gt.s32	%p5, %r5, 0;
	setp.eq.s64	%p6, %rd80, 0;
	and.pred  	%p7, %p6, %p5;
	mov.u64 	%rd79, %rd44;
	mov.u32 	%r27, %r5;
	mov.u64 	%rd84, %rd1;
	@!%p7 bra 	BB13_6;
	bra.uni 	BB13_5;

BB13_5:
	mov.u64 	%rd8, %rd84;
	mov.u64 	%rd9, %rd79;
	st.global.u64 	[%rd8], %rd44;
	ld.global.u32 	%r6, [%rd3];
	cvt.s64.s32	%rd47, %r6;
	add.s64 	%rd10, %rd8, 8;
	add.s64 	%rd11, %rd9, 1;
	setp.lt.s64	%p8, %rd11, %rd47;
	mov.u64 	%rd79, %rd11;
	mov.u32 	%r27, %r6;
	mov.u64 	%rd84, %rd10;
	@%p8 bra 	BB13_5;

BB13_6:
	mov.u32 	%r24, %r27;
	mov.u32 	%r28, %r24;
	setp.lt.s32	%p9, %r28, 1;
	@%p9 bra 	BB13_9;

	cvt.s64.s32	%rd49, %r5;
	mul.lo.s64 	%rd50, %rd80, %rd49;
	shl.b64 	%rd51, %rd50, 3;
	add.s64 	%rd81, %rd2, %rd51;
	mov.u64 	%rd85, 0;
	mov.u64 	%rd83, %rd1;

BB13_8:
	ld.global.f64 	%fd1, [%rd83];
	ld.global.f64 	%fd2, [%rd81];
	add.f64 	%fd3, %fd2, %fd1;
	st.global.f64 	[%rd83], %fd3;
	ld.global.u32 	%r28, [%rd3];
	cvt.s64.s32	%rd52, %r28;
	add.s64 	%rd83, %rd83, 8;
	add.s64 	%rd81, %rd81, 8;
	add.s64 	%rd85, %rd85, 1;
	setp.lt.s64	%p10, %rd85, %rd52;
	@%p10 bra 	BB13_8;

BB13_9:
	mov.u32 	%r29, %r28;
	add.s64 	%rd80, %rd80, 1;
	setp.lt.s64	%p11, %rd80, %rd6;
	@%p11 bra 	BB13_4;
	bra.uni 	BB13_20;

BB13_10:
	ld.global.u32 	%r30, [%rd4];
	cvt.s64.s32	%rd53, %r30;
	setp.ge.s64	%p12, %rd5, %rd53;
	@%p12 bra 	BB13_20;

	mov.u32 	%r21, %nctaid.x;
	mul.lo.s32 	%r22, %r21, %r3;
	setp.eq.s32	%p13, %r22, 16384;
	@%p13 bra 	BB13_13;

	mov.u64 	%rd54, $str;
	cvta.global.u64 	%rd55, %rd54;
	mov.u64 	%rd56, $str1;
	cvta.global.u64 	%rd57, %rd56;
	mov.u64 	%rd58, __T22;
	cvta.global.u64 	%rd59, %rd58;
	mov.u32 	%r23, 214;
	mov.u64 	%rd60, 1;
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd55;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b32 param2;
	st.param.b32	[param2+0], %r23;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd59;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd60;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 2
	ld.global.u32 	%r30, [%rd4];

BB13_13:
	ld.global.u32 	%r31, [%rd3];
	cvt.s64.s32	%rd61, %r30;
	add.s64 	%rd87, %rd5, 16384;
	setp.ge.s64	%p14, %rd87, %rd61;
	@%p14 bra 	BB13_20;

	cvt.s64.s32	%rd63, %r31;
	add.s64 	%rd22, %rd5, 16384;
	mul.lo.s64 	%rd66, %rd5, %rd63;
	shl.b64 	%rd67, %rd66, 3;
	add.s64 	%rd23, %rd2, %rd67;
	mov.u64 	%rd86, 0;

BB13_15:
	setp.lt.s32	%p15, %r31, 1;
	@%p15 bra 	BB13_19;

	shl.b64 	%rd69, %rd86, 14;
	add.s64 	%rd70, %rd22, %rd69;
	shl.b64 	%rd71, %rd70, 3;
	add.s64 	%rd72, %rd2, %rd71;
	cvt.s64.s32	%rd73, %r31;
	mul.lo.s64 	%rd74, %rd5, %rd73;
	shl.b64 	%rd75, %rd74, 3;
	add.s64 	%rd89, %rd72, %rd75;
	mov.u64 	%rd90, 0;
	mov.u64 	%rd88, %rd23;

BB13_17:
	mov.u64 	%rd27, %rd88;
	ld.global.f64 	%fd4, [%rd27];
	ld.global.f64 	%fd5, [%rd89];
	add.f64 	%fd6, %fd5, %fd4;
	st.global.f64 	[%rd27], %fd6;
	ld.global.u32 	%r31, [%rd3];
	cvt.s64.s32	%rd76, %r31;
	add.s64 	%rd89, %rd89, 8;
	add.s64 	%rd31, %rd27, 8;
	add.s64 	%rd90, %rd90, 1;
	setp.lt.s64	%p16, %rd90, %rd76;
	mov.u64 	%rd88, %rd31;
	@%p16 bra 	BB13_17;

	ld.global.u32 	%r30, [%rd4];

BB13_19:
	cvt.s64.s32	%rd77, %r30;
	add.s64 	%rd87, %rd87, 16384;
	setp.lt.s64	%p17, %rd87, %rd77;
	add.s64 	%rd86, %rd86, 1;
	@%p17 bra 	BB13_15;

BB13_20:
	ret;
}

	// .globl	blockReduce
.visible .entry blockReduce(
	.param .u64 blockReduce_param_0,
	.param .u64 blockReduce_param_1,
	.param .u64 blockReduce_param_2,
	.param .u64 blockReduce_param_3
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<372>;
	.reg .f64 	%fd<105>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd36, [blockReduce_param_0];
	ld.param.u64 	%rd37, [blockReduce_param_1];
	ld.param.u64 	%rd38, [blockReduce_param_2];
	ld.param.u64 	%rd35, [blockReduce_param_3];
	cvta.to.global.u64 	%rd1, %rd38;
	cvta.to.global.u64 	%rd2, %rd37;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r3, %r16, %r1, %r2;
	cvta.to.global.u64 	%rd39, %rd36;
	ld.global.u32 	%r17, [%rd39];
	cvt.s64.s32	%rd3, %r17;
	setp.ge.s32	%p1, %r3, %r17;
	@%p1 bra 	BB14_24;

	cvta.to.global.u64 	%rd41, %rd35;
	ld.global.s32 	%rd4, [%rd41];
	mov.u64 	%rd67, 0;
	setp.lt.s64	%p2, %rd4, 4;
	@%p2 bra 	BB14_16;

	cvt.u64.u32	%rd5, %r3;
	and.b32  	%r4, %r2, 31;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r19, %r18, %r1;
	cvt.u64.u32	%rd6, %r19;
	add.s32 	%r20, %r3, 16;
	shr.s32 	%r21, %r20, 31;
	shr.u32 	%r22, %r21, 27;
	add.s32 	%r23, %r20, %r22;
	and.b32  	%r24, %r23, -32;
	sub.s32 	%r5, %r20, %r24;
	add.s32 	%r25, %r3, 8;
	shr.s32 	%r26, %r25, 31;
	shr.u32 	%r27, %r26, 27;
	add.s32 	%r28, %r25, %r27;
	and.b32  	%r29, %r28, -32;
	sub.s32 	%r6, %r25, %r29;
	add.s32 	%r30, %r3, 4;
	shr.s32 	%r31, %r30, 31;
	shr.u32 	%r32, %r31, 27;
	add.s32 	%r33, %r30, %r32;
	and.b32  	%r34, %r33, -32;
	sub.s32 	%r7, %r30, %r34;
	add.s32 	%r35, %r3, 2;
	shr.s32 	%r36, %r35, 31;
	shr.u32 	%r37, %r36, 27;
	add.s32 	%r38, %r35, %r37;
	and.b32  	%r39, %r38, -32;
	sub.s32 	%r8, %r35, %r39;
	add.s32 	%r40, %r3, 1;
	shr.s32 	%r41, %r40, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r40, %r42;
	and.b32  	%r44, %r43, -32;
	sub.s32 	%r9, %r40, %r44;
	mov.u64 	%rd67, 0;

BB14_3:
	mov.f64 	%fd101, 0d0000000000000000;
	mov.f64 	%fd100, %fd101;
	mov.f64 	%fd99, %fd101;
	mov.f64 	%fd98, %fd101;
	setp.ge.s64	%p3, %rd5, %rd3;
	@%p3 bra 	BB14_6;

	mov.f64 	%fd101, 0d0000000000000000;
	mov.f64 	%fd100, %fd101;
	mov.f64 	%fd99, %fd101;
	mov.f64 	%fd98, %fd101;
	mov.u64 	%rd62, %rd5;

BB14_5:
	mov.u64 	%rd8, %rd62;
	mul.lo.s64 	%rd43, %rd8, %rd4;
	add.s64 	%rd44, %rd43, %rd67;
	shl.b64 	%rd45, %rd44, 3;
	add.s64 	%rd46, %rd2, %rd45;
	ld.global.f64 	%fd31, [%rd46];
	add.f64 	%fd98, %fd98, %fd31;
	ld.global.f64 	%fd32, [%rd46+8];
	add.f64 	%fd99, %fd99, %fd32;
	ld.global.f64 	%fd33, [%rd46+16];
	add.f64 	%fd100, %fd100, %fd33;
	ld.global.f64 	%fd34, [%rd46+24];
	add.f64 	%fd101, %fd101, %fd34;
	add.s64 	%rd9, %rd6, %rd8;
	setp.lt.s64	%p4, %rd9, %rd3;
	mov.u64 	%rd62, %rd9;
	@%p4 bra 	BB14_5;

BB14_6:
	// inline asm
	mov.b64 {%r45,%r46}, %fd98;
	// inline asm
	// inline asm
	mov.b64 {%r47,%r48}, %fd99;
	// inline asm
	// inline asm
	mov.b64 {%r49,%r50}, %fd100;
	// inline asm
	// inline asm
	mov.b64 {%r51,%r52}, %fd101;
	// inline asm
	mov.u32 	%r276, 31;
	// inline asm
	shfl.idx.b32 %r53, %r45, %r5, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r57, %r46, %r5, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r61, %r47, %r5, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r65, %r48, %r5, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r69, %r49, %r5, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r73, %r50, %r5, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r77, %r51, %r5, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r81, %r52, %r5, %r276;
	// inline asm
	// inline asm
	mov.b64 %fd39, {%r53,%r57};
	// inline asm
	// inline asm
	mov.b64 %fd40, {%r61,%r65};
	// inline asm
	// inline asm
	mov.b64 %fd41, {%r69,%r73};
	// inline asm
	// inline asm
	mov.b64 %fd42, {%r77,%r81};
	// inline asm
	add.f64 	%fd43, %fd98, %fd39;
	add.f64 	%fd44, %fd99, %fd40;
	add.f64 	%fd45, %fd100, %fd41;
	add.f64 	%fd46, %fd101, %fd42;
	// inline asm
	mov.b64 {%r93,%r94}, %fd43;
	// inline asm
	// inline asm
	mov.b64 {%r95,%r96}, %fd44;
	// inline asm
	// inline asm
	mov.b64 {%r97,%r98}, %fd45;
	// inline asm
	// inline asm
	mov.b64 {%r99,%r100}, %fd46;
	// inline asm
	// inline asm
	shfl.idx.b32 %r101, %r93, %r6, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r105, %r94, %r6, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r109, %r95, %r6, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r113, %r96, %r6, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r117, %r97, %r6, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r121, %r98, %r6, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r125, %r99, %r6, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r129, %r100, %r6, %r276;
	// inline asm
	// inline asm
	mov.b64 %fd47, {%r101,%r105};
	// inline asm
	// inline asm
	mov.b64 %fd48, {%r109,%r113};
	// inline asm
	// inline asm
	mov.b64 %fd49, {%r117,%r121};
	// inline asm
	// inline asm
	mov.b64 %fd50, {%r125,%r129};
	// inline asm
	add.f64 	%fd51, %fd43, %fd47;
	add.f64 	%fd52, %fd44, %fd48;
	add.f64 	%fd53, %fd45, %fd49;
	add.f64 	%fd54, %fd46, %fd50;
	// inline asm
	mov.b64 {%r141,%r142}, %fd51;
	// inline asm
	// inline asm
	mov.b64 {%r143,%r144}, %fd52;
	// inline asm
	// inline asm
	mov.b64 {%r145,%r146}, %fd53;
	// inline asm
	// inline asm
	mov.b64 {%r147,%r148}, %fd54;
	// inline asm
	// inline asm
	shfl.idx.b32 %r149, %r141, %r7, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r153, %r142, %r7, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r157, %r143, %r7, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r161, %r144, %r7, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r165, %r145, %r7, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r169, %r146, %r7, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r173, %r147, %r7, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r177, %r148, %r7, %r276;
	// inline asm
	// inline asm
	mov.b64 %fd55, {%r149,%r153};
	// inline asm
	// inline asm
	mov.b64 %fd56, {%r157,%r161};
	// inline asm
	// inline asm
	mov.b64 %fd57, {%r165,%r169};
	// inline asm
	// inline asm
	mov.b64 %fd58, {%r173,%r177};
	// inline asm
	add.f64 	%fd59, %fd51, %fd55;
	add.f64 	%fd60, %fd52, %fd56;
	add.f64 	%fd61, %fd53, %fd57;
	add.f64 	%fd62, %fd54, %fd58;
	// inline asm
	mov.b64 {%r189,%r190}, %fd59;
	// inline asm
	// inline asm
	mov.b64 {%r191,%r192}, %fd60;
	// inline asm
	// inline asm
	mov.b64 {%r193,%r194}, %fd61;
	// inline asm
	// inline asm
	mov.b64 {%r195,%r196}, %fd62;
	// inline asm
	// inline asm
	shfl.idx.b32 %r197, %r189, %r8, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r201, %r190, %r8, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r205, %r191, %r8, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r209, %r192, %r8, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r213, %r193, %r8, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r217, %r194, %r8, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r221, %r195, %r8, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r225, %r196, %r8, %r276;
	// inline asm
	// inline asm
	mov.b64 %fd63, {%r197,%r201};
	// inline asm
	// inline asm
	mov.b64 %fd64, {%r205,%r209};
	// inline asm
	// inline asm
	mov.b64 %fd65, {%r213,%r217};
	// inline asm
	// inline asm
	mov.b64 %fd66, {%r221,%r225};
	// inline asm
	add.f64 	%fd67, %fd59, %fd63;
	add.f64 	%fd68, %fd60, %fd64;
	add.f64 	%fd69, %fd61, %fd65;
	add.f64 	%fd70, %fd62, %fd66;
	// inline asm
	mov.b64 {%r237,%r238}, %fd67;
	// inline asm
	// inline asm
	mov.b64 {%r239,%r240}, %fd68;
	// inline asm
	// inline asm
	mov.b64 {%r241,%r242}, %fd69;
	// inline asm
	// inline asm
	mov.b64 {%r243,%r244}, %fd70;
	// inline asm
	// inline asm
	shfl.idx.b32 %r245, %r237, %r9, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r249, %r238, %r9, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r253, %r239, %r9, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r257, %r240, %r9, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r261, %r241, %r9, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r265, %r242, %r9, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r269, %r243, %r9, %r276;
	// inline asm
	// inline asm
	shfl.idx.b32 %r273, %r244, %r9, %r276;
	// inline asm
	// inline asm
	mov.b64 %fd71, {%r245,%r249};
	// inline asm
	// inline asm
	mov.b64 %fd72, {%r253,%r257};
	// inline asm
	// inline asm
	mov.b64 %fd73, {%r261,%r265};
	// inline asm
	// inline asm
	mov.b64 %fd74, {%r269,%r273};
	// inline asm
	add.f64 	%fd13, %fd67, %fd71;
	add.f64 	%fd14, %fd68, %fd72;
	add.f64 	%fd15, %fd69, %fd73;
	add.f64 	%fd16, %fd70, %fd74;
	setp.ne.s32	%p5, %r4, 0;
	@%p5 bra 	BB14_15;

	shl.b64 	%rd47, %rd67, 3;
	add.s64 	%rd10, %rd1, %rd47;
	ld.global.u64 	%rd63, [%rd10];

BB14_8:
	mov.u64 	%rd12, %rd63;
	mov.b64 	 %fd75, %rd12;
	add.f64 	%fd76, %fd13, %fd75;
	mov.b64 	 %rd48, %fd76;
	atom.global.cas.b64 	%rd63, [%rd10], %rd12, %rd48;
	setp.ne.s64	%p6, %rd12, %rd63;
	@%p6 bra 	BB14_8;

	ld.global.u64 	%rd64, [%rd10+8];

BB14_10:
	mov.u64 	%rd15, %rd64;
	add.s64 	%rd49, %rd10, 8;
	mov.b64 	 %fd77, %rd15;
	add.f64 	%fd78, %fd14, %fd77;
	mov.b64 	 %rd50, %fd78;
	atom.global.cas.b64 	%rd64, [%rd49], %rd15, %rd50;
	setp.ne.s64	%p7, %rd15, %rd64;
	@%p7 bra 	BB14_10;

	ld.global.u64 	%rd65, [%rd10+16];

BB14_12:
	mov.u64 	%rd18, %rd65;
	add.s64 	%rd51, %rd10, 16;
	mov.b64 	 %fd79, %rd18;
	add.f64 	%fd80, %fd15, %fd79;
	mov.b64 	 %rd52, %fd80;
	atom.global.cas.b64 	%rd65, [%rd51], %rd18, %rd52;
	setp.ne.s64	%p8, %rd18, %rd65;
	@%p8 bra 	BB14_12;

	ld.global.u64 	%rd66, [%rd10+24];

BB14_14:
	mov.u64 	%rd21, %rd66;
	add.s64 	%rd53, %rd10, 24;
	mov.b64 	 %fd81, %rd21;
	add.f64 	%fd82, %fd16, %fd81;
	mov.b64 	 %rd54, %fd82;
	atom.global.cas.b64 	%rd66, [%rd53], %rd21, %rd54;
	setp.ne.s64	%p9, %rd21, %rd66;
	@%p9 bra 	BB14_14;

BB14_15:
	add.s64 	%rd67, %rd67, 4;
	sub.s64 	%rd55, %rd4, %rd67;
	setp.gt.s64	%p10, %rd55, 3;
	@%p10 bra 	BB14_3;

BB14_16:
	setp.ge.s64	%p11, %rd67, %rd4;
	@%p11 bra 	BB14_24;

	cvt.u64.u32	%rd25, %r3;
	and.b32  	%r10, %r2, 31;
	mov.u32 	%r285, %nctaid.x;
	mul.lo.s32 	%r286, %r285, %r1;
	cvt.u64.u32	%rd26, %r286;
	add.s32 	%r287, %r3, 16;
	shr.s32 	%r288, %r287, 31;
	shr.u32 	%r289, %r288, 27;
	add.s32 	%r290, %r287, %r289;
	and.b32  	%r291, %r290, -32;
	sub.s32 	%r11, %r287, %r291;
	add.s32 	%r292, %r3, 8;
	shr.s32 	%r293, %r292, 31;
	shr.u32 	%r294, %r293, 27;
	add.s32 	%r295, %r292, %r294;
	and.b32  	%r296, %r295, -32;
	sub.s32 	%r12, %r292, %r296;
	add.s32 	%r297, %r3, 4;
	shr.s32 	%r298, %r297, 31;
	shr.u32 	%r299, %r298, 27;
	add.s32 	%r300, %r297, %r299;
	and.b32  	%r301, %r300, -32;
	sub.s32 	%r13, %r297, %r301;
	add.s32 	%r302, %r3, 2;
	shr.s32 	%r303, %r302, 31;
	shr.u32 	%r304, %r303, 27;
	add.s32 	%r305, %r302, %r304;
	and.b32  	%r306, %r305, -32;
	sub.s32 	%r14, %r302, %r306;
	add.s32 	%r307, %r3, 1;
	shr.s32 	%r308, %r307, 31;
	shr.u32 	%r309, %r308, 27;
	add.s32 	%r310, %r307, %r309;
	and.b32  	%r311, %r310, -32;
	sub.s32 	%r15, %r307, %r311;

BB14_18:
	mov.f64 	%fd103, 0d0000000000000000;
	mov.f64 	%fd104, %fd103;
	setp.ge.s64	%p12, %rd25, %rd3;
	mov.u64 	%rd68, %rd25;
	@%p12 bra 	BB14_20;

BB14_19:
	mov.u64 	%rd28, %rd68;
	mul.lo.s64 	%rd56, %rd28, %rd4;
	add.s64 	%rd57, %rd56, %rd67;
	shl.b64 	%rd58, %rd57, 3;
	add.s64 	%rd59, %rd2, %rd58;
	ld.global.f64 	%fd85, [%rd59];
	add.f64 	%fd104, %fd104, %fd85;
	add.s64 	%rd29, %rd26, %rd28;
	setp.lt.s64	%p13, %rd29, %rd3;
	mov.u64 	%rd68, %rd29;
	mov.f64 	%fd103, %fd104;
	@%p13 bra 	BB14_19;

BB14_20:
	// inline asm
	mov.b64 {%r312,%r313}, %fd103;
	// inline asm
	mov.u32 	%r369, 31;
	// inline asm
	shfl.idx.b32 %r314, %r312, %r11, %r369;
	// inline asm
	// inline asm
	shfl.idx.b32 %r318, %r313, %r11, %r369;
	// inline asm
	// inline asm
	mov.b64 %fd87, {%r314,%r318};
	// inline asm
	add.f64 	%fd88, %fd103, %fd87;
	// inline asm
	mov.b64 {%r324,%r325}, %fd88;
	// inline asm
	// inline asm
	shfl.idx.b32 %r326, %r324, %r12, %r369;
	// inline asm
	// inline asm
	shfl.idx.b32 %r330, %r325, %r12, %r369;
	// inline asm
	// inline asm
	mov.b64 %fd89, {%r326,%r330};
	// inline asm
	add.f64 	%fd90, %fd88, %fd89;
	// inline asm
	mov.b64 {%r336,%r337}, %fd90;
	// inline asm
	// inline asm
	shfl.idx.b32 %r338, %r336, %r13, %r369;
	// inline asm
	// inline asm
	shfl.idx.b32 %r342, %r337, %r13, %r369;
	// inline asm
	// inline asm
	mov.b64 %fd91, {%r338,%r342};
	// inline asm
	add.f64 	%fd92, %fd90, %fd91;
	// inline asm
	mov.b64 {%r348,%r349}, %fd92;
	// inline asm
	// inline asm
	shfl.idx.b32 %r350, %r348, %r14, %r369;
	// inline asm
	// inline asm
	shfl.idx.b32 %r354, %r349, %r14, %r369;
	// inline asm
	// inline asm
	mov.b64 %fd93, {%r350,%r354};
	// inline asm
	add.f64 	%fd94, %fd92, %fd93;
	// inline asm
	mov.b64 {%r360,%r361}, %fd94;
	// inline asm
	// inline asm
	shfl.idx.b32 %r362, %r360, %r15, %r369;
	// inline asm
	// inline asm
	shfl.idx.b32 %r366, %r361, %r15, %r369;
	// inline asm
	// inline asm
	mov.b64 %fd95, {%r362,%r366};
	// inline asm
	setp.ne.s32	%p14, %r10, 0;
	@%p14 bra 	BB14_23;

	shl.b64 	%rd60, %rd67, 3;
	add.s64 	%rd30, %rd1, %rd60;
	add.f64 	%fd22, %fd94, %fd95;
	ld.global.u64 	%rd69, [%rd30];

BB14_22:
	mov.u64 	%rd32, %rd69;
	mov.b64 	 %fd96, %rd32;
	add.f64 	%fd97, %fd22, %fd96;
	mov.b64 	 %rd61, %fd97;
	atom.global.cas.b64 	%rd69, [%rd30], %rd32, %rd61;
	setp.ne.s64	%p15, %rd32, %rd69;
	@%p15 bra 	BB14_22;

BB14_23:
	add.s64 	%rd67, %rd67, 1;
	setp.lt.s64	%p16, %rd67, %rd4;
	@%p16 bra 	BB14_18;

BB14_24:
	ret;
}

	// .globl	mapAll
.visible .entry mapAll(
	.param .u64 mapAll_param_0,
	.param .u64 mapAll_param_1,
	.param .u64 mapAll_param_2,
	.param .u64 mapAll_param_3,
	.param .u64 mapAll_param_4,
	.param .u64 mapAll_param_5
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<34>;
	.reg .f64 	%fd<58>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd19, [mapAll_param_0];
	ld.param.u64 	%rd14, [mapAll_param_1];
	ld.param.u64 	%rd15, [mapAll_param_2];
	ld.param.u64 	%rd16, [mapAll_param_3];
	ld.param.u64 	%rd17, [mapAll_param_4];
	ld.param.u64 	%rd18, [mapAll_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.u32 	%r13, [%rd20];
	setp.ge.s32	%p1, %r4, %r13;
	@%p1 bra 	BB15_10;

	cvta.to.global.u64 	%rd21, %rd18;
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd22, %rd15;
	mul.wide.s32 	%rd23, %r4, 8;
	add.s64 	%rd24, %rd22, %rd23;
	ld.global.f64 	%fd1, [%rd24];
	ld.global.u32 	%r5, [%rd21];
	mov.f64 	%fd56, 0d0000000000000000;
	setp.lt.s32	%p2, %r5, 1;
	@%p2 bra 	BB15_4;

	cvta.to.global.u64 	%rd28, %rd17;
	mul.lo.s32 	%r16, %r5, %r4;
	mul.wide.s32 	%rd25, %r16, 8;
	add.s64 	%rd29, %rd1, %rd25;
	mov.f64 	%fd56, 0d0000000000000000;
	mov.u32 	%r32, 0;

BB15_3:
	ld.global.f64 	%fd13, [%rd29];
	ld.global.f64 	%fd14, [%rd28];
	fma.rn.f64 	%fd56, %fd14, %fd13, %fd56;
	add.s64 	%rd29, %rd29, 8;
	add.s64 	%rd28, %rd28, 8;
	add.s32 	%r32, %r32, 1;
	setp.lt.s32	%p3, %r32, %r5;
	@%p3 bra 	BB15_3;

BB15_4:
	mul.f64 	%fd5, %fd1, %fd56;
	neg.f64 	%fd15, %fd5;
	mov.f64 	%fd16, 0d4338000000000000;
	mov.f64 	%fd17, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd18, %fd15, %fd17, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r8, %temp}, %fd18;
	}
	mov.f64 	%fd19, 0dC338000000000000;
	add.rn.f64 	%fd20, %fd18, %fd19;
	mov.f64 	%fd21, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd22, %fd20, %fd21, %fd15;
	mov.f64 	%fd23, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd24, %fd20, %fd23, %fd22;
	mov.f64 	%fd25, 0d3E928AF3FCA213EA;
	mov.f64 	%fd26, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0d3F81111111122322;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3FA55555555502A1;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	mov.f64 	%fd40, 0d3FC5555555555511;
	fma.rn.f64 	%fd41, %fd39, %fd24, %fd40;
	mov.f64 	%fd42, 0d3FE000000000000B;
	fma.rn.f64 	%fd43, %fd41, %fd24, %fd42;
	mov.f64 	%fd44, 0d3FF0000000000000;
	fma.rn.f64 	%fd45, %fd43, %fd24, %fd44;
	fma.rn.f64 	%fd46, %fd45, %fd24, %fd44;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r9, %temp}, %fd46;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r10}, %fd46;
	}
	shl.b32 	%r17, %r8, 20;
	add.s32 	%r18, %r10, %r17;
	mov.b64 	%fd57, {%r9, %r18};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd15;
	}
	mov.b32 	 %f2, %r19;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB15_7;

	setp.gt.f64	%p5, %fd5, 0d8000000000000000;
	mov.f64 	%fd47, 0d7FF0000000000000;
	sub.f64 	%fd48, %fd47, %fd5;
	selp.f64	%fd57, 0d0000000000000000, %fd48, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB15_7;

	shr.u32 	%r20, %r8, 31;
	add.s32 	%r21, %r8, %r20;
	shr.s32 	%r22, %r21, 1;
	shl.b32 	%r23, %r22, 20;
	add.s32 	%r24, %r23, %r10;
	mov.b64 	%fd49, {%r9, %r24};
	sub.s32 	%r25, %r8, %r22;
	shl.b32 	%r26, %r25, 20;
	add.s32 	%r27, %r26, 1072693248;
	mov.u32 	%r28, 0;
	mov.b64 	%fd50, {%r28, %r27};
	mul.f64 	%fd57, %fd49, %fd50;

BB15_7:
	@%p2 bra 	BB15_10;

	cvta.to.global.u64 	%rd26, %rd16;
	add.f64 	%fd51, %fd57, 0d3FF0000000000000;
	rcp.rn.f64 	%fd52, %fd51;
	add.f64 	%fd53, %fd52, 0dBFF0000000000000;
	mul.f64 	%fd10, %fd1, %fd53;
	mul.lo.s32 	%r31, %r5, %r4;
	mul.wide.s32 	%rd27, %r31, 8;
	add.s64 	%rd31, %rd26, %rd27;
	add.s64 	%rd30, %rd1, %rd27;
	mov.u32 	%r33, 0;

BB15_9:
	ld.global.f64 	%fd54, [%rd30];
	mul.f64 	%fd55, %fd10, %fd54;
	st.global.f64 	[%rd31], %fd55;
	add.s64 	%rd31, %rd31, 8;
	add.s64 	%rd30, %rd30, 8;
	add.s32 	%r33, %r33, 1;
	setp.lt.s32	%p8, %r33, %r5;
	@%p8 bra 	BB15_9;

BB15_10:
	ret;
}


