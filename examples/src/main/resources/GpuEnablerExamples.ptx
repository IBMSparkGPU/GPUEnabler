//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21313570
// Cuda compilation tools, release 8.0, V8.0.53
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_35
.address_size 64

	// .globl	sum
.extern .func __assertfail
(
	.param .b64 __assertfail_param_0,
	.param .b64 __assertfail_param_1,
	.param .b32 __assertfail_param_2,
	.param .b64 __assertfail_param_3,
	.param .b64 __assertfail_param_4
)
;
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.global .align 1 .b8 __T20[44] = {118, 111, 105, 100, 32, 115, 117, 109, 40, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 41, 0};
.global .align 1 .b8 __T21[41] = {118, 111, 105, 100, 32, 115, 117, 109, 108, 40, 105, 110, 116, 44, 32, 108, 111, 110, 103, 32, 42, 44, 32, 108, 111, 110, 103, 32, 42, 44, 32, 105, 110, 116, 44, 32, 105, 110, 116, 41, 0};
.global .align 1 .b8 __T22[48] = {118, 111, 105, 100, 32, 115, 117, 109, 108, 111, 40, 105, 110, 116, 32, 42, 44, 32, 108, 111, 110, 103, 32, 42, 44, 32, 108, 111, 110, 103, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 41, 0};
.global .align 1 .b8 $str[31] = {106, 117, 109, 112, 32, 61, 61, 32, 98, 108, 111, 99, 107, 68, 105, 109, 46, 120, 32, 42, 32, 103, 114, 105, 100, 68, 105, 109, 46, 120, 0};
.global .align 1 .b8 $str1[22] = {71, 112, 117, 69, 110, 97, 98, 108, 101, 114, 69, 120, 97, 109, 112, 108, 101, 115, 46, 99, 117, 0};
.global .align 1 .b8 $str2[34] = {67, 85, 68, 65, 32, 75, 69, 82, 78, 69, 76, 32, 65, 68, 68, 32, 37, 108, 100, 32, 43, 32, 37, 108, 100, 32, 61, 32, 37, 108, 100, 32, 10, 0};
.global .align 1 .b8 $str3[34] = {67, 85, 68, 65, 32, 75, 69, 82, 78, 69, 76, 32, 77, 85, 76, 32, 37, 108, 100, 32, 42, 32, 37, 108, 100, 32, 61, 32, 37, 108, 100, 32, 10, 0};

.visible .entry sum(
	.param .u64 sum_param_0,
	.param .u64 sum_param_1,
	.param .u64 sum_param_2,
	.param .u64 sum_param_3,
	.param .u64 sum_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd16, [sum_param_0];
	ld.param.u64 	%rd17, [sum_param_1];
	ld.param.u64 	%rd15, [sum_param_2];
	ld.param.u64 	%rd18, [sum_param_3];
	cvta.to.global.u64 	%rd39, %rd17;
	cvta.to.global.u64 	%rd2, %rd16;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd19, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd20, %r3, %r2;
	add.s64 	%rd3, %rd20, %rd19;
	cvta.to.global.u64 	%rd21, %rd18;
	ld.global.u32 	%r13, [%rd21];
	setp.eq.s32	%p1, %r13, 0;
	@%p1 bra 	BB0_5;

	setp.ne.s64	%p2, %rd3, 0;
	@%p2 bra 	BB0_12;

	ld.global.s32 	%rd23, [%rd2];
	setp.lt.s64	%p3, %rd23, 16384;
	selp.b64	%rd4, %rd23, 16384, %p3;
	mov.u32 	%r24, 0;
	mov.u32 	%r25, %r24;
	mov.u64 	%rd40, 0;
	setp.lt.s64	%p4, %rd4, 1;
	@%p4 bra 	BB0_4;

BB0_3:
	ld.global.u32 	%r16, [%rd39];
	add.s32 	%r25, %r16, %r25;
	add.s64 	%rd39, %rd39, 4;
	add.s64 	%rd40, %rd40, 1;
	setp.lt.s64	%p5, %rd40, %rd4;
	mov.u32 	%r24, %r25;
	@%p5 bra 	BB0_3;

BB0_4:
	cvta.to.global.u64 	%rd24, %rd15;
	st.global.u32 	[%rd24], %r24;
	bra.uni 	BB0_12;

BB0_5:
	ld.global.u32 	%r26, [%rd2];
	cvt.s64.s32	%rd25, %r26;
	setp.ge.s64	%p6, %rd3, %rd25;
	@%p6 bra 	BB0_12;

	mov.u32 	%r17, %nctaid.x;
	mul.lo.s32 	%r18, %r17, %r3;
	setp.eq.s32	%p7, %r18, 16384;
	@%p7 bra 	BB0_8;

	mov.u64 	%rd26, $str;
	cvta.global.u64 	%rd27, %rd26;
	mov.u64 	%rd28, $str1;
	cvta.global.u64 	%rd29, %rd28;
	mov.u64 	%rd30, __T20;
	cvta.global.u64 	%rd31, %rd30;
	mov.u32 	%r19, 10;
	mov.u64 	%rd32, 1;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd27;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd29;
	.param .b32 param2;
	st.param.b32	[param2+0], %r19;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd31;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd32;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 0
	ld.global.u32 	%r26, [%rd2];

BB0_8:
	cvt.s64.s32	%rd9, %r26;
	mov.u32 	%r27, 0;
	setp.ge.s64	%p8, %rd3, %rd9;
	@%p8 bra 	BB0_11;

	add.s64 	%rd35, %rd20, %rd19;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd41, %rd39, %rd36;
	mov.u32 	%r27, 0;
	mov.u64 	%rd42, %rd3;

BB0_10:
	mov.u64 	%rd12, %rd42;
	ld.global.u32 	%r22, [%rd41];
	add.s32 	%r27, %r22, %r27;
	add.s64 	%rd41, %rd41, 65536;
	add.s64 	%rd14, %rd12, 16384;
	setp.lt.s64	%p9, %rd14, %rd9;
	mov.u64 	%rd42, %rd14;
	@%p9 bra 	BB0_10;

BB0_11:
	shl.b64 	%rd37, %rd3, 2;
	add.s64 	%rd38, %rd39, %rd37;
	st.global.u32 	[%rd38], %r27;

BB0_12:
	ret;
}

	// .globl	suml
.visible .entry suml(
	.param .u32 suml_param_0,
	.param .u64 suml_param_1,
	.param .u64 suml_param_2,
	.param .u32 suml_param_3,
	.param .u32 suml_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<52>;


	ld.param.u32 	%r4, [suml_param_0];
	ld.param.u64 	%rd20, [suml_param_1];
	ld.param.u64 	%rd19, [suml_param_2];
	ld.param.u32 	%r5, [suml_param_3];
	cvta.to.global.u64 	%rd44, %rd20;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd21, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd22, %r3, %r2;
	add.s64 	%rd2, %rd22, %rd21;
	setp.eq.s32	%p1, %r5, 0;
	@%p1 bra 	BB1_5;

	setp.ne.s64	%p2, %rd2, 0;
	@%p2 bra 	BB1_11;

	setp.lt.s32	%p3, %r4, 16384;
	cvt.s64.s32	%rd26, %r4;
	selp.b64	%rd3, %rd26, 16384, %p3;
	mov.u64 	%rd47, 0;
	mov.u64 	%rd48, %rd47;
	mov.u64 	%rd45, %rd47;
	setp.lt.s64	%p4, %rd3, 1;
	@%p4 bra 	BB1_4;

BB1_3:
	ld.global.u64 	%rd27, [%rd44];
	add.s64 	%rd48, %rd27, %rd48;
	add.s64 	%rd44, %rd44, 8;
	add.s64 	%rd45, %rd45, 1;
	setp.lt.s64	%p5, %rd45, %rd3;
	mov.u64 	%rd47, %rd48;
	@%p5 bra 	BB1_3;

BB1_4:
	cvta.to.global.u64 	%rd28, %rd19;
	st.global.u64 	[%rd28], %rd47;
	bra.uni 	BB1_11;

BB1_5:
	cvt.s64.s32	%rd11, %r4;
	setp.ge.s64	%p6, %rd2, %rd11;
	@%p6 bra 	BB1_11;

	mov.u32 	%r6, %nctaid.x;
	mul.lo.s32 	%r7, %r6, %r3;
	setp.eq.s32	%p7, %r7, 16384;
	@%p7 bra 	BB1_8;

	mov.u64 	%rd29, $str;
	cvta.global.u64 	%rd30, %rd29;
	mov.u64 	%rd31, $str1;
	cvta.global.u64 	%rd32, %rd31;
	mov.u64 	%rd33, __T21;
	cvta.global.u64 	%rd34, %rd33;
	mov.u32 	%r8, 35;
	mov.u64 	%rd35, 1;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd30;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd32;
	.param .b32 param2;
	st.param.b32	[param2+0], %r8;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd34;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd35;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 1

BB1_8:
	add.s64 	%rd39, %rd22, %rd21;
	shl.b64 	%rd40, %rd39, 3;
	add.s64 	%rd49, %rd44, %rd40;
	mov.u64 	%rd51, 0;
	mov.u64 	%rd50, %rd2;

BB1_9:
	mov.u64 	%rd14, %rd50;
	ld.global.u64 	%rd41, [%rd49];
	add.s64 	%rd51, %rd41, %rd51;
	add.s64 	%rd49, %rd49, 131072;
	add.s64 	%rd18, %rd14, 16384;
	setp.lt.s64	%p8, %rd18, %rd11;
	mov.u64 	%rd50, %rd18;
	@%p8 bra 	BB1_9;

	shl.b64 	%rd42, %rd2, 3;
	add.s64 	%rd43, %rd44, %rd42;
	st.global.u64 	[%rd43], %rd51;

BB1_11:
	ret;
}

	// .globl	sumlo
.visible .entry sumlo(
	.param .u64 sumlo_param_0,
	.param .u64 sumlo_param_1,
	.param .u64 sumlo_param_2,
	.param .u64 sumlo_param_3,
	.param .u64 sumlo_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<59>;


	ld.param.u64 	%rd22, [sumlo_param_0];
	ld.param.u64 	%rd23, [sumlo_param_1];
	ld.param.u64 	%rd21, [sumlo_param_2];
	ld.param.u64 	%rd24, [sumlo_param_3];
	cvta.to.global.u64 	%rd51, %rd23;
	cvta.to.global.u64 	%rd2, %rd22;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd25, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd26, %r3, %r2;
	add.s64 	%rd3, %rd26, %rd25;
	cvta.to.global.u64 	%rd27, %rd24;
	ld.global.u32 	%r7, [%rd27];
	setp.eq.s32	%p1, %r7, 0;
	@%p1 bra 	BB2_5;

	setp.ne.s64	%p2, %rd3, 0;
	@%p2 bra 	BB2_12;

	ld.global.s32 	%rd31, [%rd2];
	setp.lt.s64	%p3, %rd31, 16384;
	selp.b64	%rd4, %rd31, 16384, %p3;
	mov.u64 	%rd54, 0;
	mov.u64 	%rd55, %rd54;
	mov.u64 	%rd52, %rd54;
	setp.lt.s64	%p4, %rd4, 1;
	@%p4 bra 	BB2_4;

BB2_3:
	ld.global.u64 	%rd32, [%rd51];
	add.s64 	%rd55, %rd32, %rd55;
	add.s64 	%rd51, %rd51, 8;
	add.s64 	%rd52, %rd52, 1;
	setp.lt.s64	%p5, %rd52, %rd4;
	mov.u64 	%rd54, %rd55;
	@%p5 bra 	BB2_3;

BB2_4:
	cvta.to.global.u64 	%rd33, %rd21;
	st.global.u64 	[%rd33], %rd54;
	bra.uni 	BB2_12;

BB2_5:
	ld.global.u32 	%r11, [%rd2];
	cvt.s64.s32	%rd34, %r11;
	setp.ge.s64	%p6, %rd3, %rd34;
	@%p6 bra 	BB2_12;

	mov.u32 	%r8, %nctaid.x;
	mul.lo.s32 	%r9, %r8, %r3;
	setp.eq.s32	%p7, %r9, 16384;
	@%p7 bra 	BB2_8;

	mov.u64 	%rd35, $str;
	cvta.global.u64 	%rd36, %rd35;
	mov.u64 	%rd37, $str1;
	cvta.global.u64 	%rd38, %rd37;
	mov.u64 	%rd39, __T22;
	cvta.global.u64 	%rd40, %rd39;
	mov.u32 	%r10, 60;
	mov.u64 	%rd41, 1;
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd36;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd38;
	.param .b32 param2;
	st.param.b32	[param2+0], %r10;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd40;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd41;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 2
	ld.global.u32 	%r11, [%rd2];

BB2_8:
	cvt.s64.s32	%rd12, %r11;
	mov.u64 	%rd58, 0;
	setp.ge.s64	%p8, %rd3, %rd12;
	@%p8 bra 	BB2_11;

	add.s64 	%rd46, %rd26, %rd25;
	shl.b64 	%rd47, %rd46, 3;
	add.s64 	%rd56, %rd51, %rd47;
	mov.u64 	%rd58, 0;
	mov.u64 	%rd57, %rd3;

BB2_10:
	mov.u64 	%rd15, %rd57;
	ld.global.u64 	%rd48, [%rd56];
	add.s64 	%rd58, %rd48, %rd58;
	add.s64 	%rd56, %rd56, 131072;
	add.s64 	%rd19, %rd15, 16384;
	setp.lt.s64	%p9, %rd19, %rd12;
	mov.u64 	%rd57, %rd19;
	@%p9 bra 	BB2_10;

BB2_11:
	shl.b64 	%rd49, %rd3, 3;
	add.s64 	%rd50, %rd51, %rd49;
	st.global.u64 	[%rd50], %rd58;

BB2_12:
	ret;
}

	// .globl	add
.visible .entry add(
	.param .u32 add_param_0,
	.param .u64 add_param_1,
	.param .u64 add_param_2,
	.param .u64 add_param_3
)
{
	.local .align 8 .b8 	__local_depot3[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<21>;


	mov.u64 	%rd20, __local_depot3;
	cvta.local.u64 	%SP, %rd20;
	ld.param.u32 	%r2, [add_param_0];
	ld.param.u64 	%rd1, [add_param_1];
	ld.param.u64 	%rd2, [add_param_2];
	ld.param.u64 	%rd3, [add_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB3_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.u64 	%rd9, [%rd8];
	ld.global.u64 	%rd10, [%rd6];
	add.s64 	%rd11, %rd9, %rd10;
	cvta.to.global.u64 	%rd12, %rd3;
	add.s64 	%rd13, %rd12, %rd5;
	st.global.u64 	[%rd13], %rd11;
	ld.global.u64 	%rd14, [%rd6];
	ld.global.u64 	%rd15, [%rd8];
	add.u64 	%rd16, %SP, 0;
	cvta.to.local.u64 	%rd17, %rd16;
	st.local.u64 	[%rd17], %rd14;
	st.local.u64 	[%rd17+8], %rd15;
	st.local.u64 	[%rd17+16], %rd11;
	mov.u64 	%rd18, $str2;
	cvta.global.u64 	%rd19, %rd18;
	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd19;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd16;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r6, [retval0+0];
	
	//{
	}// Callseq End 3

BB3_2:
	ret;
}

	// .globl	mul
.visible .entry mul(
	.param .u32 mul_param_0,
	.param .u64 mul_param_1,
	.param .u64 mul_param_2,
	.param .u64 mul_param_3
)
{
	.local .align 8 .b8 	__local_depot4[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<21>;


	mov.u64 	%rd20, __local_depot4;
	cvta.local.u64 	%SP, %rd20;
	ld.param.u32 	%r2, [mul_param_0];
	ld.param.u64 	%rd1, [mul_param_1];
	ld.param.u64 	%rd2, [mul_param_2];
	ld.param.u64 	%rd3, [mul_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB4_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.u64 	%rd9, [%rd8];
	ld.global.u64 	%rd10, [%rd6];
	mul.lo.s64 	%rd11, %rd9, %rd10;
	cvta.to.global.u64 	%rd12, %rd3;
	add.s64 	%rd13, %rd12, %rd5;
	st.global.u64 	[%rd13], %rd11;
	ld.global.u64 	%rd14, [%rd6];
	ld.global.u64 	%rd15, [%rd8];
	add.u64 	%rd16, %SP, 0;
	cvta.to.local.u64 	%rd17, %rd16;
	st.local.u64 	[%rd17], %rd14;
	st.local.u64 	[%rd17+8], %rd15;
	st.local.u64 	[%rd17+16], %rd11;
	mov.u64 	%rd18, $str3;
	cvta.global.u64 	%rd19, %rd18;
	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd19;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd16;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r6, [retval0+0];
	
	//{
	}// Callseq End 4

BB4_2:
	ret;
}

	// .globl	arrayTest
.visible .entry arrayTest(
	.param .u32 arrayTest_param_0,
	.param .u64 arrayTest_param_1,
	.param .u64 arrayTest_param_2,
	.param .u64 arrayTest_param_3,
	.param .u64 arrayTest_param_4,
	.param .u64 arrayTest_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<21>;


	ld.param.u32 	%r2, [arrayTest_param_0];
	ld.param.u64 	%rd1, [arrayTest_param_1];
	ld.param.u64 	%rd2, [arrayTest_param_2];
	ld.param.u64 	%rd3, [arrayTest_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB5_2;

	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.lo.s32 	%r6, %r1, 3;
	mul.wide.s32 	%rd7, %r6, 8;
	add.s64 	%rd8, %rd5, %rd7;
	mul.wide.s32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd6, %rd9;
	ld.global.u64 	%rd11, [%rd10];
	ld.global.u64 	%rd12, [%rd8];
	mul.lo.s64 	%rd13, %rd11, %rd12;
	add.s64 	%rd14, %rd4, %rd7;
	st.global.u64 	[%rd14], %rd13;
	ld.global.u64 	%rd15, [%rd10];
	ld.global.u64 	%rd16, [%rd8+8];
	mul.lo.s64 	%rd17, %rd15, %rd16;
	st.global.u64 	[%rd14+8], %rd17;
	ld.global.u64 	%rd18, [%rd10];
	ld.global.u64 	%rd19, [%rd8+16];
	mul.lo.s64 	%rd20, %rd18, %rd19;
	st.global.u64 	[%rd14+16], %rd20;

BB5_2:
	ret;
}

	// .globl	multiplyBy2_self
.visible .entry multiplyBy2_self(
	.param .u32 multiplyBy2_self_param_0,
	.param .u64 multiplyBy2_self_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<7>;


	ld.param.u32 	%r2, [multiplyBy2_self_param_0];
	ld.param.u64 	%rd1, [multiplyBy2_self_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB6_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 8;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.u64 	%rd5, [%rd4];
	shl.b64 	%rd6, %rd5, 1;
	st.global.u64 	[%rd4], %rd6;

BB6_2:
	ret;
}

	// .globl	multiplyBy2
.visible .entry multiplyBy2(
	.param .u32 multiplyBy2_param_0,
	.param .u64 multiplyBy2_param_1,
	.param .u64 multiplyBy2_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r2, [multiplyBy2_param_0];
	ld.param.u64 	%rd1, [multiplyBy2_param_1];
	ld.param.u64 	%rd2, [multiplyBy2_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB7_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u64 	%rd6, [%rd5];
	shl.b64 	%rd7, %rd6, 1;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd4;
	st.global.u64 	[%rd9], %rd7;

BB7_2:
	ret;
}

	// .globl	multiplyBy2o
.visible .entry multiplyBy2o(
	.param .u64 multiplyBy2o_param_0,
	.param .u64 multiplyBy2o_param_1,
	.param .u64 multiplyBy2o_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd3, [multiplyBy2o_param_0];
	ld.param.u64 	%rd1, [multiplyBy2o_param_1];
	ld.param.u64 	%rd2, [multiplyBy2o_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r1, %r2, %r3, %r4;
	ld.global.u32 	%r5, [%rd4];
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB8_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u64 	%rd8, [%rd7];
	shl.b64 	%rd9, %rd8, 1;
	cvta.to.global.u64 	%rd10, %rd2;
	add.s64 	%rd11, %rd10, %rd6;
	st.global.u64 	[%rd11], %rd9;

BB8_2:
	ret;
}

	// .globl	load
.visible .entry load(
	.param .u32 load_param_0,
	.param .u64 load_param_1
)
{



	ret;
}


